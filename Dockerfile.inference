# syntax = docker/dockerfile:experimental
FROM pytorch/torchserve:0.3.0-cpu as base

USER root

FROM base as reqs
COPY gcp/inference/requirements.txt requirements.txt
RUN pip3 install --upgrade pip
RUN pip install -r requirements.txt

FROM reqs as build-torchserve
COPY gcp/inference/handler.py /home/model-server
COPY data/models/*.pt /home/model-server

WORKDIR /home/model-server

ARG MODELS
RUN for m in $MODELS; \
do torch-model-archiver \
    --model-name $m \
    --version 1.0 \
    --serialized-file $m.pt \
    --handler handler.py \
    --export-path=model-store; \
done

ADD gcp/inference/start.sh /usr/local/bin/start.sh
RUN chmod 777 /usr/local/bin/start.sh
ENV MODELS ${MODELS}
CMD ["/usr/local/bin/start.sh", "\"${MODELS}\""]


